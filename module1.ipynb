{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "def area_ratio(gdf, jinguan_class): #斑块面积比重\n",
    "    gdf2 = gdf[gdf['CLASS_NAME'] == jinguan_class]\n",
    "    return gdf2['area'].sum() / gdf['area'].sum()\n",
    "\n",
    "def SHDI(p1, p2, p3, p4):   #香农多样性指数\n",
    "    min_p = 1e-10\n",
    "    return -(p1 * math.log(max(p1, min_p)) + p2 * math.log(max(p2, min_p)) + p3 * math.log(max(p3, min_p))  + p4 * math.log(max(p4, min_p)))\n",
    "\n",
    "def SHEI(p1, p2, p3, p4):   #香农均匀性指数\n",
    "    if SHDI(p1, p2, p3, p4) == 0:\n",
    "        return 1\n",
    "    p = [p1, p2, p3, p4]\n",
    "    S = sum(1 for item in p if item != 0)\n",
    "    return SHDI(p1, p2, p3, p4) / math.log(S)\n",
    "\n",
    "def max_index(gdf, jinguan_class): #最大斑块指数\n",
    "    gdf2 = gdf[gdf['CLASS_NAME'] == jinguan_class]   \n",
    "    if len(gdf2) == 0: return 0\n",
    "    return gdf2['area'].max() / gdf['area'].sum()\n",
    "\n",
    "def fragmentation(gdf, jinguan_class):   #破碎度\n",
    "    gdf2 = gdf[gdf['CLASS_NB1ME'] == jinguan_class]    \n",
    "    return (gdf2.shape[0] - 1) * gdf2['area'].min() / gdf2['area'].sum()\n",
    "\n",
    "def density(gdf, jinguan_class): #斑块密度\n",
    "    gdf2 = gdf[gdf['CLASS_NAME'] == jinguan_class]    \n",
    "    return len(gdf2)\n",
    "#     return len(gdf2) / gdf['area'].sum() * 1e6\n",
    "\n",
    "def average_distance(gdf, jinguan_class):\n",
    "    gdf2 = gdf[gdf['CLASS_NAME'] == jinguan_class].copy()\n",
    "    if len(gdf2) == 1: return 0\n",
    "    internal_points = gdf2.representative_point()\n",
    "    # 将中心点添加到原始GeoDataFrame中\n",
    "    gdf2['internal_point'] = internal_points\n",
    "    distances = []\n",
    "    # 计算所有点对之间的距离\n",
    "    for i in gdf2.index:\n",
    "        for j in gdf2.index:\n",
    "            if i < j:  # 只计算每一对点一次，避免重复\n",
    "                dist = gdf2.geometry[i].distance(gdf2.geometry[j])\n",
    "                distances.append(dist)\n",
    "    return (sum(distances) / math.sqrt(gdf['area'].sum())) / len(distances)\n",
    "#     return sum(distances)/len(distances)\n",
    "\n",
    "def separate(gdf, jinguan_class):  #分离度\n",
    "    gdf2 = gdf[gdf['CLASS_NAME'] == jinguan_class].copy()\n",
    "    result = 0\n",
    "    for index, row in gdf2.iterrows():\n",
    "        result += (row['area'] / gdf2['area'].sum()) * (row['area'] / gdf2['area'].sum())\n",
    "    return 1 - result\n",
    "\n",
    "def edge_density(gdf, jinguan_class):   #边缘密度\n",
    "    gdf2 = gdf[gdf['CLASS_NAME'] == jinguan_class]\n",
    "    gdf2 = gdf2.to_crs(epsg=4547)\n",
    "    # 初始化边长总和和面积总和\n",
    "    total_perimeter = 0\n",
    "    total_area = 0\n",
    "    # 遍历所有几何对象\n",
    "    for geometry in gdf2.geometry:\n",
    "        # 如果是复多边形，则分解为多个多边形\n",
    "        if isinstance(geometry, MultiPolygon):\n",
    "            for polygon in geometry.geoms:\n",
    "                total_perimeter += polygon.length\n",
    "                total_area += polygon.area\n",
    "        elif isinstance(geometry, Polygon):\n",
    "            total_perimeter += geometry.length\n",
    "            total_area += geometry.area\n",
    "    if len(gdf2) == 0: return 0\n",
    "    return total_perimeter / total_area\n",
    "\n",
    "def mediumland_area_ratio(gdf, jinguan_class):\n",
    "    gdf2 = gdf[gdf['CLASS_NAME'] == jinguan_class]\n",
    "    k = gdf['area'].sum()\n",
    "    gdf3 = gdf2[gdf2['area'] > 0.05 * k]\n",
    "    gdf4 = gdf3[gdf3['area'] < 0.2 * k]\n",
    "    return gdf4['area'].sum() / gdf2['area'].sum()\n",
    "\n",
    "def island_area_ratio(gdf, jinguan_class):\n",
    "    gdf2 = gdf[gdf['CLASS_NAME'] == jinguan_class]\n",
    "    k = gdf['area'].sum()\n",
    "    gdf3 = gdf2[gdf2['area'] < 0.04 * k]\n",
    "    return gdf3['area'].sum() / gdf2['area'].sum()\n",
    "\n",
    "def translate(in_, label_class):\n",
    "    if type(in_) == str:\n",
    "        if  label_class == 'A':\n",
    "            if in_ == '景观单一均质': return 1\n",
    "            if in_ == '山林景观均质': return 2\n",
    "            if in_ == '农业景观均质': return 3\n",
    "            if in_ == '城镇化景观均质': return 4\n",
    "            if in_ == '景观多样异质': return 5\n",
    "        if  label_class == 'B1':\n",
    "            if in_ == '聚落连片集中': return 1\n",
    "            if in_ == '聚落分散集中': return 2\n",
    "            if in_ == '聚落分散破碎': return 3 \n",
    "        if  label_class == 'B2':\n",
    "            if in_ == '耕地连片集中': return 1\n",
    "            if in_ == '耕地分散集中': return 2\n",
    "            if in_ == '耕地分散破碎': return 3 \n",
    "        if  label_class == 'C':\n",
    "            if in_ == '聚落团状': return 1\n",
    "            if in_ == '聚落带状': return 2\n",
    "            if in_ == '聚落自由状': return 3 \n",
    "            if in_ == '聚落形状相似，以自由状为主': return 4\n",
    "            if in_ == '聚落形状相似，以带状为主': return 5\n",
    "            if in_ == '聚落形状相似，以团状为主': return 6\n",
    "            if in_ == '聚落形状分异': return 7          \n",
    "    else:\n",
    "        if  label_class == 'A':\n",
    "            if in_ == 1: return '景观单一均质'\n",
    "            if in_ == 2: return '山林景观均质'\n",
    "            if in_ == 3: return '农业景观均质'\n",
    "            if in_ == 4: return '城镇化景观均质'\n",
    "            if in_ == 5: return '景观多样异质'\n",
    "        if  label_class == 'B1':\n",
    "            if in_ == 1: return '聚落连片集中'\n",
    "            if in_ == 2: return '聚落分散集中'\n",
    "            if in_ == 3: return '聚落分散破碎'\n",
    "        if  label_class == 'B2':\n",
    "            if in_ == 1: return '耕地连片集中'\n",
    "            if in_ == 2: return '耕地分散集中'\n",
    "            if in_ == 3: return '耕地分散破碎'\n",
    "        if  label_class == 'C':\n",
    "            if in_ == 1: return '聚落团状'\n",
    "            if in_ == 2: return '聚落带状'\n",
    "            if in_ == 3: return '聚落自由状'\n",
    "            if in_ == 4: return '聚落形状相似，以自由状为主'\n",
    "            if in_ == 5: return '聚落形状相似，以带状为主'\n",
    "            if in_ == 6: return '聚落形状相似，以团状为主'\n",
    "            if in_ == 7: return '聚落形状分异'\n",
    "        \n",
    "def get_label_value(row, label_class, k):           # eweewewew\n",
    "    if pd.notnull(row['real' + label_class]):\n",
    "        label_vlaue = row['real' + label_class]\n",
    "    else:\n",
    "        # 使用mode()方法，并处理多个模式的情况（如果有并列最多的元素，只取第一个）\n",
    "        label_vlaue = row[['tester1' + label_class, 'tester2' + label_class, 'tester3' + label_class, 'tester4' + label_class]].mode()[0]\n",
    "    if row['tester1' + label_class] == label_vlaue: k += 1\n",
    "    if row['tester2' + label_class] == label_vlaue: k += 1\n",
    "    if row['tester3' + label_class] == label_vlaue: k += 1\n",
    "    if row['tester4' + label_class] == label_vlaue: k += 1\n",
    "    return translate(label_vlaue, label_class), k\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义MLP模型\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def cal(train_X, train_y, test_X, test_y, get_model = False):    \n",
    "    # 训练模型\n",
    "    decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "    decision_tree.fit(train_X, train_y)\n",
    "    #预测\n",
    "    prediction_label1 = decision_tree.predict(test_X)\n",
    "    accuracy1 = np.count_nonzero(prediction_label1 == test_y) / len(test_y)\n",
    "#     print(f'Test Accuracy1: {accuracy1:.4f}')\n",
    "    \n",
    "\n",
    "    # 训练模型\n",
    "    svc = SVC(kernel='linear')  # 使用线性核\n",
    "    svc.fit(train_X, train_y)\n",
    "    #预测\n",
    "    prediction_label2 = svc.predict(test_X)\n",
    "    accuracy2 = np.count_nonzero(prediction_label2 == test_y) / len(test_y)\n",
    "#     print(f'Test Accuracy2: {accuracy2:.4f}')\n",
    "\n",
    "    \n",
    "    \n",
    "    # 训练模型\n",
    "    logistic_regression = LogisticRegression(max_iter = 10000)\n",
    "    logistic_regression.fit(train_X, train_y)\n",
    "    #预测\n",
    "    prediction_label3 = logistic_regression.predict(test_X)\n",
    "    accuracy3 = np.count_nonzero(prediction_label3 == test_y) / len(test_y)\n",
    "#     print(f'Test Accuracy3: {accuracy3:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    # 训练模型\n",
    "    random_forest = RandomForestClassifier(n_estimators=100, random_state=42)  # 100棵树\n",
    "    random_forest.fit(train_X, train_y)\n",
    "    #预测\n",
    "    prediction_label4 = random_forest.predict(test_X)\n",
    "    accuracy4 = np.count_nonzero(prediction_label4 == test_y) / len(test_y)\n",
    "\n",
    "    # # 获取特征重要性\n",
    "    # importances = random_forest.feature_importances_\n",
    "    # column_names = ['cmbh', 'Label_A', 'Label_B1', 'Label_B2', 'Label_C（无效）']\n",
    "    # column_names += ['p1（建设用地的面积占比）', 'p2（耕地的面积占比）', 'p3（林地的面积占比）', 'p4（草地的面积占比）']\n",
    "    # column_names += ['m1（建设用地的最大斑块指数）', 'm2（耕地的最大斑块指数）', 'm3（林地的最大斑块指数）', 'm4（草地的最大斑块指数）']\n",
    "    # column_names += ['SHDI（香农多样性指数）', 'SHEI（香农均匀性指数）']\n",
    "    # column_names += ['den1（建设用地的斑块数量）', 'medland1（中型建筑用地斑块的面积占比）', 'island1（小型建设用地斑块的面积占比）', 'sep1（建设用地的分离度）']\n",
    "    # column_names += ['den2（耕地的斑块数量）', 'medland2（中型耕地斑块的面积占比）', 'island1（小型耕地斑块的面积占比）', 'sep1（耕地的分离度）']\n",
    "    # column_names += ['水域的面积占比', '水域的最大斑块指数']\n",
    "    # column_names += ['林地的分离度', '草地的分离度', '水域的分离度']\n",
    "    # column_names += ['建设用地的边缘密度', '耕地的边缘密度', '林地的边缘密度', '草地的边缘密度', '水域的边缘密度']\n",
    "    # feature_names = []\n",
    "    # for i in [6, 19, 20, 21, 22]: feature_names.append(column_names[i])\n",
    "    # feature_importances_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    # if not os.path.isfile('out.xlsx'):\n",
    "    #     # 创建一个空的DataFrame，并写入Excel文件，以创建文件\n",
    "    #     feature_importances_df.to_excel('out.xlsx',sheet_name='labelB2', index=False)\n",
    "    # with pd.ExcelWriter('out.xlsx', engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "    #     feature_importances_df.to_excel(writer, sheet_name='lableB2', index=False)\n",
    "# unlabeled_X_A = all_unlabeled_informations[:,[5, 6, 7, 9, 10, 11, 13, 14]].copy()\n",
    "# unlabeled_X_B1 = all_unlabeled_informations[:,[5, 15, 16, 17, 18]].copy()\n",
    "# unlabeled_X_B2 = all_unlabeled_informations[:,[6, 19, 20, 21, 22]].copy()\n",
    "\n",
    "\n",
    "    # 设置模型参数\n",
    "    input_size = train_X.shape[1]\n",
    "    hidden_size = 8\n",
    "    output_size = int(train_y.max())  # 五分类问题\n",
    "\n",
    "    # 创建模型实例\n",
    "    model = SimpleMLP(input_size, hidden_size, output_size)\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "    # 假设data是您的二维numpy数组\n",
    "    # 将numpy数组转换为pytorch张量\n",
    "    train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
    "    # 对于多分类问题，标签需要转换为整数索引\n",
    "    train_y_tensor = torch.tensor(train_y - 1, dtype=torch.long)\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 3000\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_X_tensor)\n",
    "        loss = criterion(outputs, train_y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         if (epoch+1) % 3000 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    test_X_tensor = torch.tensor(test_X, dtype=torch.float32)\n",
    "    test_y_tensor = torch.tensor(test_y - 1, dtype=torch.long)\n",
    "\n",
    "    # 将模型设置为评估模式\n",
    "    model.eval()\n",
    "\n",
    "    # 禁用梯度计算，以减少计算量\n",
    "    with torch.no_grad():\n",
    "        # 对测试集进行预测\n",
    "        outputs_test = model(test_X_tensor)\n",
    "        # 获取预测结果的最大值所在的位置，即预测的类别\n",
    "        prediction_label5_tensor = outputs_test.argmax(dim=1)\n",
    "\n",
    "    # 计算正确率\n",
    "    accuracy5 = (prediction_label5_tensor == test_y_tensor).float().mean()\n",
    "    accuracy5 = accuracy5.item()\n",
    "#     print(f'Test Accuracy5: {accuracy5:.4f}')\n",
    "\n",
    "    prediction_labels = np.vstack((prediction_label1, prediction_label2, prediction_label3, prediction_label4, prediction_label5_tensor.numpy() + 1)).T\n",
    "\n",
    "    # 使用Pandas的mode函数来找到每列中出现次数最多的值\n",
    "    super_prediction_label = (pd.DataFrame(prediction_labels).mode(axis=1)[0]).values\n",
    "    accuracy6 = np.count_nonzero(super_prediction_label == test_y) / len(test_y) \n",
    "#     print(f'Test Accuracy6: {accuracy6:.4f}')\n",
    "\n",
    "\n",
    "    k1 = 0\n",
    "    index = np.where((super_prediction_label == test_y) == False)[0]\n",
    "    for i in range(0, prediction_labels.shape[0]):\n",
    "        _, counts = np.unique(prediction_labels[i], return_counts=True)\n",
    "        if counts.max() > 3: k1+=1\n",
    "#     print(prediction_labels.shape[0], k1)\n",
    "\n",
    "    k2 = 0\n",
    "    for i in index:\n",
    "        _, counts = np.unique(prediction_labels[i], return_counts=True)\n",
    "        if counts.max() > 3: k2+=1\n",
    "#     print(len(index), k2)\n",
    "    accuracy7 = ((k1 - k2) / k1)\n",
    "#     print(f'Test Accuracy7: {accuracy7:.4f}')\n",
    "    if get_model: \n",
    "        print(accuracy1, accuracy2, accuracy3, accuracy4, accuracy5, accuracy6, accuracy7)\n",
    "        return decision_tree, svc, logistic_regression, random_forest, model\n",
    "    return accuracy1, accuracy2, accuracy3, accuracy4, accuracy5, accuracy6, accuracy7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def str_to_list(s):\n",
    "    s = str(s)\n",
    "    # 使用正则表达式找到所有的数字\n",
    "    numbers = re.findall(r'\\d+', s.replace('，', ','))\n",
    "    # 将字符串数字转换为整数列表\n",
    "    numbers_list = [int(num) for num in numbers]\n",
    "    return numbers_list[1:]\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # 减去每行的最大值以防止溢出\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def diffsion(all_unlabeled_informations, all_edges):\n",
    "    k = 0\n",
    "    pointer = 0\n",
    "    diff_values = np.zeros((0, 10))\n",
    "    for area_id, area in enumerate(excel_areas):\n",
    "        all_edge = all_edges[area_id]\n",
    "        all_unlabeled_information = all_unlabeled_informations[pointer:pointer + len(all_edge)]\n",
    "        pointer += len(all_edge)\n",
    "        diff_value = np.zeros((all_unlabeled_information.shape[0], 10))\n",
    "        for i in range(0, all_unlabeled_information.shape[0]):\n",
    "            current_information = all_unlabeled_information[i]\n",
    "            for nearby_cmbh in all_edge[current_information[0]]:\n",
    "                if all_unlabeled_information[all_unlabeled_information[:,0] == nearby_cmbh].shape[0] != 1: continue\n",
    "                if nearby_cmbh == current_information[0]: continue\n",
    "                nearby_information = all_unlabeled_information[all_unlabeled_information[:,0] == nearby_cmbh][0]\n",
    "                if  np.sum(abs(nearby_information[5:9] - current_information[5:9])) < 0.2:\n",
    "                    k += 1\n",
    "                    diff_value[all_unlabeled_information[:,0] == nearby_cmbh] += current_information[5:15] * -0.1\n",
    "                    diff_value[i] += nearby_information[5:15] * 0.1\n",
    "        diff_values = np.vstack((diff_values, diff_value))\n",
    "    print('扩散次数：',k)\n",
    "    all_unlabeled_informations[:,5:15] += diff_values\n",
    "    return all_unlabeled_informations\n",
    "\n",
    "\n",
    "def get_label(test_X, model1, model2, model3, model4, model5, get_all = False):\n",
    "    prediction_label1 = model1.predict(test_X)  \n",
    "    prediction_label2 = model2.predict(test_X)  \n",
    "    prediction_label3 = model3.predict(test_X)  \n",
    "    prediction_label4 = model4.predict(test_X)\n",
    "    test_X_tensor = torch.tensor(test_X, dtype=torch.float32)    \n",
    "    with torch.no_grad():\n",
    "        # 对测试集进行预测\n",
    "        outputs_test = model5(test_X_tensor)\n",
    "        # 获取预测结果的最大值所在的位置，即预测的类别\n",
    "        prediction_label5_tensor = outputs_test.argmax(dim=1)\n",
    "\n",
    "    prediction_label5 = prediction_label5_tensor.numpy() + 1\n",
    "    prediction_labels = np.vstack((prediction_label1, prediction_label2, prediction_label3, prediction_label4, prediction_label5)).T\n",
    "    \n",
    "    # 使用Pandas的mode函数来找到每列中出现次数最多的值\n",
    "    super_prediction_label = (pd.DataFrame(prediction_labels).mode(axis=1)[0]).values\n",
    "    if not get_all: return super_prediction_label\n",
    "    return prediction_label1, prediction_label2, prediction_label3, prediction_label4, prediction_label5, super_prediction_label\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "def cal_xgb(train_X, train_y, test_X, test_y, get_model = False):\n",
    "    dtrain = xgb.DMatrix(train_X, label=train_y - 1)\n",
    "    dtest = xgb.DMatrix(test_X, label=test_y - 1)\n",
    "    params = {\n",
    "        'max_depth': 3,         # 树的最大深度\n",
    "        'eta': 0.1,             # 学习率\n",
    "        'objective': 'binary:logistic' if len(np.unique(train_y)) == 2 else 'multi:softmax',  # 目标函数\n",
    "        'num_class': len(np.unique(train_y))  # 类别数量，仅对多分类问题有效\n",
    "    }\n",
    "    num_round = 100  # 迭代次数\n",
    "    bst = xgb.train(params, dtrain, num_round)\n",
    "    preds = bst.predict(dtest)\n",
    "    pred_labels = preds + 1\n",
    "    accuracy = accuracy_score(test_y, pred_labels)\n",
    "    if get_model: return bst\n",
    "    return accuracy\n",
    "\n",
    "def get_label_xgb(test_X, bst):\n",
    "    dtest = xgb.DMatrix(test_X)\n",
    "    preds = bst.predict(dtest)\n",
    "    pred_labels = preds + 1\n",
    "    return pred_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 24) (1058, 34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\44156\\anaconda3\\envs\\deeplearn\\lib\\site-packages\\pyogrio\\raw.py:198: RuntimeWarning: C:/Users/44156/Desktop/科研项目/空间基因/data/杭州村落/杭州村筛选0606-2.shp contains polygon(s) with rings with invalid winding order. Autocorrecting them, but that shapefile should be corrected using ogr2ogr for example.\n",
      "  return ogr_read(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 24) (1696, 34)\n",
      "(173, 24) (2898, 34)\n",
      "(246, 24) (4060, 34)\n",
      "(299, 24) (9345, 34)\n",
      "(376, 24) (11377, 34)\n",
      "(437, 24) (12532, 34)\n",
      "(483, 24) (13258, 34)\n",
      "0.8219461697722568 0.7883022774327122 0.7013457556935818\n"
     ]
    }
   ],
   "source": [
    "# 定义Excel文件名\n",
    "excel_files = ['tester1', 'tester2', 'tester3', 'tester4']\n",
    "excel_areas = ['常州','杭州', '湖州', '嘉兴', '上海','苏州','无锡','镇江']\n",
    "label_classes = ['A', 'B1', 'B2', 'C']\n",
    "\n",
    "#定义一些数据\n",
    "all_labeled_informations = np.zeros((0, 24))\n",
    "k1 = 0                                              #记录下人工正确率\n",
    "k2 = 0\n",
    "k3 = 0\n",
    "k4 = 0\n",
    "all_unlabeled_informations = np.zeros((0, 24 + 10))\n",
    "all_edges = []\n",
    "\n",
    "for area_id, area in enumerate(excel_areas):\n",
    "    # 读取Excel文件并存储在字典中\n",
    "    dataframes = []\n",
    "    for file in excel_files:\n",
    "        df = pd.read_excel('C:/Users/44156/Desktop/科研项目/空间基因/data_label/' + area + '/' + area + file + '.xlsx', usecols=[0, 1 ,2 ,3, 4])\n",
    "        # 假设样本编号列的名称是 'SampleID'，标签列的名称是 'tester'\n",
    "        df.columns = ['SampleID', file +'A', file +'B1', file +'B2', file +'C']\n",
    "        dataframes.append(df)\n",
    "       \n",
    "    for label_class in label_classes:\n",
    "        df = pd.read_excel('C:/Users/44156/Desktop/科研项目/空间基因/data_label/修正标签0624/' + label_class + '.xlsx', sheet_name=area,  usecols=[0, 7])\n",
    "        df.columns = ['SampleID', 'real' + label_class]\n",
    "        dataframes.append(df)\n",
    "    # 合并所有DataFrame\n",
    "    merged_df = dataframes[0].copy()\n",
    "    for dataframe in dataframes[1:]:\n",
    "        merged_df = merged_df.merge(dataframe, on='SampleID', how='outer')\n",
    "    unique_values = merged_df['SampleID'].unique()\n",
    "    \n",
    "    \n",
    "    all_labeled_information = np.zeros((len(merged_df), 24))   \n",
    "                               \n",
    "    # 设定用地的目录路径\n",
    "    directory_path ='C:/Users/44156/Desktop/科研项目/空间基因/data/' + area +'用地'\n",
    "    # 列出目录下所有的文件\n",
    "    file_list = os.listdir(directory_path)\n",
    "    # 筛选出所有的.shp文件\n",
    "    shp_files = [f for f in file_list if f.endswith('.shp')]\n",
    "    # 检查是否只有一个.shp文件\n",
    "    if len(shp_files) == 1:\n",
    "        # 获取唯一的.shp文件名\n",
    "        shp_file = shp_files[0]\n",
    "        # 读取.shp文件\n",
    "        gdf_xc = gpd.read_file(directory_path + '/' + shp_file)\n",
    "        gdf_xc = gdf_xc[gdf_xc['CLASS_NAME'].isin(['林地','耕地' ,'草地', '建设用地', '水域'])]\n",
    "    else:\n",
    "        print(\"没有找到唯一的.shp文件，或者找到了多个.shp文件。跳过该地区：\" + area)  \n",
    "        break\n",
    "     \n",
    "    # 设定村落的目录路径\n",
    "    directory_path ='C:/Users/44156/Desktop/科研项目/空间基因/data/' + area +'村落'\n",
    "    # 列出目录下所有的文件\n",
    "    file_list = os.listdir(directory_path)\n",
    "    # 筛选出所有的.shp文件\n",
    "    shp_files = [f for f in file_list if f.endswith('.shp')]\n",
    "    # 检查是否只有一个.shp文件\n",
    "    if len(shp_files) == 1:\n",
    "        # 获取唯一的.shp文件名\n",
    "        shp_file = shp_files[0]\n",
    "        # 读取.shp文件\n",
    "        gdf_cunluo = gpd.read_file(directory_path + '/' + shp_file)\n",
    "    else:\n",
    "        print(\"没有找到唯一的.shp文件，或者找到了多个.shp文件。跳过该地区：\" + area)  \n",
    "        break\n",
    "    \n",
    "    for index, row in merged_df.iterrows():\n",
    "        all_labeled_information[index][0] = row['SampleID']           \n",
    "        gdf_xc_filtered = gdf_xc[gdf_xc['cmbh'] == row['SampleID']]\n",
    "        all_labeled_information[index][1], k1 = get_label_value(row, 'A', k1)\n",
    "        all_labeled_information[index][2], k2 = get_label_value(row, 'B1', k2)\n",
    "        all_labeled_information[index][3], k3 = get_label_value(row, 'B2', k3)\n",
    "        all_labeled_information[index][4], k4 = get_label_value(row, 'C', k4)\n",
    "        p1 = area_ratio(gdf_xc_filtered, '建设用地')\n",
    "        p2 = area_ratio(gdf_xc_filtered, '耕地')\n",
    "        p3 = area_ratio(gdf_xc_filtered, '林地')\n",
    "        p4 = area_ratio(gdf_xc_filtered, '草地')\n",
    "        all_labeled_information[index][5] = p1\n",
    "        all_labeled_information[index][6] = p2\n",
    "        all_labeled_information[index][7] = p3\n",
    "        all_labeled_information[index][8] = p4\n",
    "        all_labeled_information[index][9] = max_index(gdf_xc_filtered, '建设用地')\n",
    "        all_labeled_information[index][10] = max_index(gdf_xc_filtered, '耕地')\n",
    "        all_labeled_information[index][11] = max_index(gdf_xc_filtered, '林地')\n",
    "        all_labeled_information[index][12] = max_index(gdf_xc_filtered,  '草地')\n",
    "        SH = SHDI(p1, p2, p3, p4)\n",
    "        all_labeled_information[index][13] = SH\n",
    "        SH = SHEI(p1, p2, p3, p4)\n",
    "        all_labeled_information[index][14] = SH\n",
    "        if len(gdf_xc_filtered[gdf_xc_filtered['CLASS_NAME'] == '建设用地']) == 0:\n",
    "            d1 = 0\n",
    "            m1 = 0\n",
    "            i1 = 0\n",
    "            s1 = 0\n",
    "        else:\n",
    "            d1 = density(gdf_xc_filtered, '建设用地')\n",
    "            m1 = mediumland_area_ratio(gdf_xc_filtered, '建设用地')\n",
    "            i1 = island_area_ratio(gdf_xc_filtered, '建设用地')\n",
    "            s1 = separate(gdf_xc_filtered, '建设用地')\n",
    "        if len(gdf_xc_filtered[gdf_xc_filtered['CLASS_NAME'] == '耕地']) == 0:\n",
    "            d2 = 0\n",
    "            m2 = 0\n",
    "            i2 = 0\n",
    "            s2 = 0\n",
    "        else:\n",
    "            d2 = density(gdf_xc_filtered, '耕地')\n",
    "            m2 = mediumland_area_ratio(gdf_xc_filtered, '耕地')\n",
    "            i2 = island_area_ratio(gdf_xc_filtered, '耕地')\n",
    "            s2 = separate(gdf_xc_filtered, '耕地')\n",
    "        all_labeled_information[index][15] = d1\n",
    "        all_labeled_information[index][16] = m1\n",
    "        all_labeled_information[index][17] = i1\n",
    "        all_labeled_information[index][18] = s1\n",
    "        all_labeled_information[index][19] = d2\n",
    "        all_labeled_information[index][20] = m2\n",
    "        all_labeled_information[index][21] = i2\n",
    "        all_labeled_information[index][22] = s2\n",
    "        all_labeled_information[index][23] = area_id\n",
    "    \n",
    "    unique_values = gdf_xc['cmbh'].unique()\n",
    "    all_unlabeled_information = np.zeros((len(unique_values), 24 + 10))\n",
    "    all_edge = {}\n",
    "    for index, cmbh in enumerate(unique_values):\n",
    "        all_edge[cmbh] = str_to_list(gdf_cunluo[gdf_cunluo['cmbh'] == cmbh]['相邻cmbh'])\n",
    "        \n",
    "        gdf_xc_filtered = gdf_xc[gdf_xc['cmbh'] == cmbh]\n",
    "        all_unlabeled_information[index][0] = cmbh\n",
    "        all_unlabeled_information[index][1] = 0\n",
    "        all_unlabeled_information[index][2] = 0\n",
    "        all_unlabeled_information[index][3] = 0\n",
    "        all_unlabeled_information[index][4] = 0\n",
    "        p1 = area_ratio(gdf_xc_filtered, '建设用地')\n",
    "        p2 = area_ratio(gdf_xc_filtered, '耕地')\n",
    "        p3 = area_ratio(gdf_xc_filtered, '林地')\n",
    "        p4 = area_ratio(gdf_xc_filtered, '草地')\n",
    "        all_unlabeled_information[index][5] = p1\n",
    "        all_unlabeled_information[index][6] = p2\n",
    "        all_unlabeled_information[index][7] = p3\n",
    "        all_unlabeled_information[index][8] = p4\n",
    "        all_unlabeled_information[index][9] = max_index(gdf_xc_filtered, '建设用地')\n",
    "        all_unlabeled_information[index][10] = max_index(gdf_xc_filtered, '耕地')\n",
    "        all_unlabeled_information[index][11] = max_index(gdf_xc_filtered, '林地')\n",
    "        all_unlabeled_information[index][12] = max_index(gdf_xc_filtered,  '草地')\n",
    "        SH = SHDI(p1, p2, p3, p4)\n",
    "        all_unlabeled_information[index][13] = SH\n",
    "        SH = SHEI(p1, p2, p3, p4)\n",
    "        all_unlabeled_information[index][14] = SH\n",
    "        if len(gdf_xc_filtered[gdf_xc_filtered['CLASS_NAME'] == '建设用地']) == 0:\n",
    "            d1 = 0\n",
    "            m1 = 0\n",
    "            i1 = 0\n",
    "            s1 = 0\n",
    "        else:\n",
    "            d1 = density(gdf_xc_filtered, '建设用地')\n",
    "            m1 = mediumland_area_ratio(gdf_xc_filtered, '建设用地')\n",
    "            i1 = island_area_ratio(gdf_xc_filtered, '建设用地')\n",
    "            s1 = separate(gdf_xc_filtered, '建设用地')\n",
    "        if len(gdf_xc_filtered[gdf_xc_filtered['CLASS_NAME'] == '耕地']) == 0:\n",
    "            d2 = 0\n",
    "            m2 = 0\n",
    "            i2 = 0\n",
    "            s2 = 0\n",
    "        else:\n",
    "            d2 = density(gdf_xc_filtered, '耕地')\n",
    "            m2 = mediumland_area_ratio(gdf_xc_filtered, '耕地')\n",
    "            i2 = island_area_ratio(gdf_xc_filtered, '耕地')\n",
    "            s2 = separate(gdf_xc_filtered, '耕地')\n",
    "        all_unlabeled_information[index][15] = d1\n",
    "        all_unlabeled_information[index][16] = m1\n",
    "        all_unlabeled_information[index][17] = i1\n",
    "        all_unlabeled_information[index][18] = s1\n",
    "        all_unlabeled_information[index][19] = d2\n",
    "        all_unlabeled_information[index][20] = m2\n",
    "        all_unlabeled_information[index][21] = i2\n",
    "        all_unlabeled_information[index][22] = s2\n",
    "        all_unlabeled_information[index][23] = area_id\n",
    "\n",
    "        ## 额外的指标，\n",
    "        all_unlabeled_information[index][24] = area_ratio(gdf_xc_filtered, '水域')\n",
    "        all_unlabeled_information[index][25] = max_index(gdf_xc_filtered, '水域')\n",
    "        all_unlabeled_information[index][26] = separate(gdf_xc_filtered, '林地')\n",
    "        all_unlabeled_information[index][27] = separate(gdf_xc_filtered, '草地')\n",
    "        all_unlabeled_information[index][28] = separate(gdf_xc_filtered, '水域')\n",
    "        all_unlabeled_information[index][29] = edge_density(gdf_xc_filtered, '建设用地')\n",
    "        all_unlabeled_information[index][30] = edge_density(gdf_xc_filtered, '耕地')\n",
    "        all_unlabeled_information[index][31] = edge_density(gdf_xc_filtered, '林地')\n",
    "        all_unlabeled_information[index][32] = edge_density(gdf_xc_filtered, '草地')\n",
    "        all_unlabeled_information[index][33] = edge_density(gdf_xc_filtered, '水域')       \n",
    "\n",
    "    all_labeled_informations = np.vstack((all_labeled_informations, all_labeled_information))\n",
    "    all_unlabeled_informations = np.vstack((all_unlabeled_informations, all_unlabeled_information))\n",
    "    all_edges.append(all_edge)\n",
    "    print(all_labeled_informations.shape, all_unlabeled_informations.shape)\n",
    "print(k1 / all_labeled_informations.shape[0] / 4, k2 / all_labeled_informations.shape[0] / 4, k3 / all_labeled_informations.shape[0] / 4)\n",
    "        \n",
    "                                                                                   \n",
    "                                                                                   \n",
    "                                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.8758620689655172 0.8482758620689655 1.0 0.9103448390960693 0.9448275862068966 1.0\n",
      "1.0 0.8 0.8 1.0 0.7793103456497192 0.8206896551724138 1.0\n",
      "1.0 0.8275862068965517 0.8137931034482758 1.0 0.8275862336158752 0.8620689655172413 1.0\n"
     ]
    }
   ],
   "source": [
    "#写入指标\n",
    "\n",
    "split_ratio = 0.7\n",
    "\n",
    "# 计算分割点\n",
    "split_index = int(all_labeled_informations.shape[0] * split_ratio)\n",
    "\n",
    "# 分割数组\n",
    "part1 = all_labeled_informations[:split_index]\n",
    "# part1[:, 6] /= part1[:, 6].mean()\n",
    "part2 = all_labeled_informations[split_index:]\n",
    "\n",
    "train_X_A = all_labeled_informations[:,[5, 6, 7, 9, 10, 11, 13, 14]].copy()\n",
    "train_y_A = all_labeled_informations[:,1].copy()\n",
    "train_X_B1 = all_labeled_informations[:,[5, 15, 16, 17, 18]].copy()\n",
    "train_y_B1 = all_labeled_informations[:,2].copy()\n",
    "train_X_B2 = all_labeled_informations[:,[6, 19, 20, 21, 22]].copy()\n",
    "train_y_B2 = all_labeled_informations[:,3].copy()\n",
    "\n",
    "test_X_A = part2[:,[5, 6, 7, 9, 10, 11, 13, 14]]\n",
    "test_y_A = part2[:,1]\n",
    "test_X_B1 = part2[:,[5, 15, 16, 17, 18]]\n",
    "test_y_B1 = part2[:,2]\n",
    "test_X_B2 = part2[:,[6, 19, 20, 21, 22]]\n",
    "test_y_B2 = part2[:,3]\n",
    "\n",
    "unlabeled_X_A = all_unlabeled_informations[:,[5, 6, 7, 9, 10, 11, 13, 14]].copy()\n",
    "unlabeled_X_B1 = all_unlabeled_informations[:,[5, 15, 16, 17, 18]].copy()\n",
    "unlabeled_X_B2 = all_unlabeled_informations[:,[6, 19, 20, 21, 22]].copy()\n",
    "\n",
    "model1, model2, model3, model4, model5 = cal(train_X_A, train_y_A, test_X_A, test_y_A, True)\n",
    "all_unlabeled_informations[:,1] = get_label(unlabeled_X_A, model1, model2, model3, model4, model5)\n",
    "\n",
    "model1, model2, model3, model4, model5 = cal(train_X_B1, train_y_B1, test_X_B1, test_y_B1, True)\n",
    "all_unlabeled_informations[:,2] = get_label(unlabeled_X_B1, model1, model2, model3, model4, model5)\n",
    "\n",
    "model1, model2, model3, model4, model5 = cal(train_X_B2, train_y_B2, test_X_B2, test_y_B2, True)\n",
    "all_unlabeled_informations[:,3] = get_label(unlabeled_X_B2, model1, model2, model3, model4, model5)\n",
    "\n",
    "column_names = ['cmbh', 'Label_A', 'Label_B1', 'Label_B2', 'Label_C（无效）']\n",
    "column_names += ['p1（建设用地的面积占比）', 'p2（耕地的面积占比）', 'p3（林地的面积占比）', 'p4（草地的面积占比）']\n",
    "column_names += ['m1（建设用地的最大斑块指数）', 'm2（耕地的最大斑块指数）', 'm3（林地的最大斑块指数）', 'm4（草地的最大斑块指数）']\n",
    "column_names += ['SHDI（香农多样性指数）', 'SHEI（香农均匀性指数）']\n",
    "column_names += ['den1（建设用地的斑块数量）', 'medland1（中型建筑用地斑块的面积占比）', 'island1（小型建设用地斑块的面积占比）', 'sep1（建设用地的分离度）']\n",
    "column_names += ['den2（耕地的斑块数量）', 'medland2（中型耕地斑块的面积占比）', 'island1（小型耕地斑块的面积占比）', 'sep1（耕地的分离度）']\n",
    "column_names += ['水域的面积占比', '水域的最大斑块指数']\n",
    "column_names += ['林地的分离度', '草地的分离度', '水域的分离度']\n",
    "column_names += ['建设用地的边缘密度', '耕地的边缘密度', '林地的边缘密度', '草地的边缘密度', '水域的边缘密度']\n",
    "\n",
    "all_unlabeled_informations = np.round(all_unlabeled_informations, 4)\n",
    "\n",
    "with pd.ExcelWriter('output.xlsx') as writer:\n",
    "    for i in range(len(excel_areas)):\n",
    "        index =  all_unlabeled_informations[:,23] == i\n",
    "        output_indicators = pd.DataFrame(np.hstack((all_unlabeled_informations[index,0:23], all_unlabeled_informations[index,24:])), columns=column_names)\n",
    "        output_indicators.to_excel(writer, sheet_name=excel_areas[i], index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
